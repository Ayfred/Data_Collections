{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read credentials\n",
    "f = open('credentials.json')\n",
    "data = json.load(f)\n",
    "f.close()\n",
    "\n",
    "consumer_key = data[\"consumer_key\"] #Your API/Consumer key \n",
    "consumer_secret = data[\"consumer_secret\"] #Your API/Consumer Secret Key\n",
    "access_token = data[\"access_token\"]    #Your Access token key\n",
    "access_token_secret = data[\"access_token_secret\"] #Your Access token Secret key\n",
    "\n",
    "# Connect to twitter API\n",
    "auth = tweepy.OAuth1UserHandler(\n",
    "    consumer_key, consumer_secret,\n",
    "    access_token, access_token_secret\n",
    ")\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = \"john\"\n",
    "no_of_tweets =10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search by tweeter user name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The number of tweets we want to retrieved from the user\n",
    "tweets = api.user_timeline(screen_name=user, count=no_of_tweets)\n",
    "#tweets = tweepy.Cursor(api.user_timeline, screen_name = user, count = no_of_tweets, tweet_mode = 'extended').items(no_of_tweets)\n",
    "\n",
    "#Pulling Some attributes from the tweet\n",
    "attributes_container = [[tweet.created_at, tweet.favorite_count,tweet.source,  tweet.text] for tweet in tweets]\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#Creation of column list to rename the columns in the dataframe\n",
    "columns = [\"Date Created\", \"Number of Likes\", \"Source of Tweet\", \"Tweet\"]\n",
    "    \n",
    "#Creation of Dataframe\n",
    "tweets_dataFrame = pd.DataFrame(attributes_container, columns=columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search by keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = 'GPT-3'\n",
    "no_of_tweets = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                user                                              Tweet\n",
      "0        TGKThunders  (3/3)\\n\\nAlso $GPT is being launched by @TheDa...\n",
      "1       Compound_Cap  RT @solomania: The stages of playing with GPT-...\n",
      "2       yumemizi18_1  異世界召喚はチャットGPTと共に〜現代チートAIを異世界にぶち込んでみた〜『第３話「ギルド『...\n",
      "3           DamianS1  RT @serpstat: 🔥Today is the day!\\nUnlock the p...\n",
      "4         CacofonixI  RT @solomania: The stages of playing with GPT-...\n",
      "..               ...                                                ...\n",
      "995      thereklund1  RT @NoCap_Capital: AI has been a hot topic rec...\n",
      "996   Mencarialasan3  RT @jstcallmepapi: Wen OG\\n\\nToday I am going ...\n",
      "997  rinchanblog0215  AIチャットボット（GPT-3）とスプレッドシートを連携する方法 https://t.co/...\n",
      "998     chenxuan4074  RT @juanli324: 最近大家都在谈论、关注ChatGPT。究竟GPT的确切意思是什...\n",
      "999    Rahmeljackson  RT @ChrisCroy: @Rahmeljackson Yeah that's thei...\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "tweets = tweepy.Cursor(api.search_tweets, q = keywords, count = no_of_tweets, tweet_mode = 'extended').items(no_of_tweets)\n",
    "\n",
    "data = []\n",
    "columns = ['user', 'Tweet']\n",
    "\n",
    "for tweet in tweets : \n",
    "    data.append([tweet.user.screen_name, tweet.full_text])\n",
    "\n",
    "df = pd.DataFrame(data, columns = columns)\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading openpyxl-3.1.1-py2.py3-none-any.whl (249 kB)\n",
      "     ---------------------------------------- 0.0/249.8 kB ? eta -:--:--\n",
      "     -------------------------- ----------- 174.1/249.8 kB 5.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 249.8/249.8 kB 5.1 MB/s eta 0:00:00\n",
      "Collecting et-xmlfile\n",
      "  Using cached et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(r'D:\\maxim\\Documents\\Projects\\CollecteDesDonnees\\UserKeywordsGPT.xlsx', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English language & French language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                user                                              Tweet\n",
      "0    HumbleTechMicro  Open $AI GPT integration by NLU company listed...\n",
      "1    RandompileVideo  Preprint on #bioRxiv: https://t.co/BbRfEzjxt5\\...\n",
      "2          jeffkagan  RT @JohnNosta: AI combines disparate and eclec...\n",
      "3         heyitsnoah  Lots of folks - like @natfriedman https://t.co...\n",
      "4        PamelaTamby  @davidlisnard Des deux... dans le sens où les ...\n",
      "..               ...                                                ...\n",
      "995    diogomiguel_9  🤖 ChatGPT can write ✍️ flow expressions, \\n✅ M...\n",
      "996       Gangster5x  @MyMetaTrader #mymetatrader\\n\\n🔥 Trade smarter...\n",
      "997        JohnNosta  GPT offers us the opportunity to customize pat...\n",
      "998       andrew__io  #GPT3 tip of the day [bot]: Always strive to r...\n",
      "999  RandompileVideo  Preprint on #bioRxiv: https://t.co/yiBqQnfEPz\\...\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "tweets = tweepy.Cursor(api.search_tweets, q = \"GPT3 lang:en OR lang:fr\", count = no_of_tweets, tweet_mode = 'extended').items(no_of_tweets)\n",
    "\n",
    "data = []\n",
    "columns = ['user', 'Tweet']\n",
    "\n",
    "for tweet in tweets : \n",
    "    data.append([tweet.user.screen_name, tweet.full_text])\n",
    "\n",
    "df = pd.DataFrame(data, columns = columns)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(r'D:\\maxim\\Documents\\Projects\\CollecteDesDonnees\\Twitter\\UserKeywordsGPT.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'likes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32md:\\Program Fil\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32md:\\Program Fil\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\Program Fil\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'likes'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m time_likes \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mSeries(data \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mlikes\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mvalues, index \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m time_likes\u001b[39m.\u001b[39mplot(figsize\u001b[39m=\u001b[39m(\u001b[39m16\u001b[39m,\u001b[39m4\u001b[39m), color \u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[1;32md:\\Program Fil\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:3804\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3802\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3804\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3806\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32md:\\Program Fil\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3806\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3807\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3810\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'likes'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "time_likes = pd.Series(data = df['likes'].values, index = df['date'])\n",
    "time_likes.plot(figsize=(16,4), color ='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in d:\\program fil\\python\\python311\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in d:\\program fil\\python\\python311\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in d:\\program fil\\python\\python311\\lib\\site-packages (from nltk>=3.1->textblob) (8.1.3)\n",
      "Requirement already satisfied: joblib in d:\\program fil\\python\\python311\\lib\\site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\program fil\\python\\python311\\lib\\site-packages (from nltk>=3.1->textblob) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in d:\\program fil\\python\\python311\\lib\\site-packages (from nltk>=3.1->textblob) (4.64.1)\n",
      "Requirement already satisfied: colorama in d:\\program fil\\python\\python311\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_sentiment(tweet):\n",
    "    analysis = TextBlob(clean_tweet(tweet))\n",
    "\n",
    "    if analysis.sentiment.polarity > 0: return 1\n",
    "    elif analysis.sentiment.polarity == 0: return 0\n",
    "    else : return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      Interesting. Shows that GPT was trained on git...\n",
      "1      AI combines disparate and eclectic concepts to...\n",
      "2      RT @shubroski: This weekend I built =GPT3(), a...\n",
      "3      RT @CodeSavvyOrg: \"...developers still must fu...\n",
      "4      This article about chatGPT looks interesting a...\n",
      "                             ...                        \n",
      "995    RT @JingfengY: #ChatGPT and #GPT3 are hot. But...\n",
      "996    First out// $AMST: Amesite Announces GPT-3 Pow...\n",
      "997    RT @JingfengY: #ChatGPT and #GPT3 are hot. But...\n",
      "998    Preprint on #bioRxiv: https://t.co/yHPm96Kq6N\\...\n",
      "999    @DataChaz Maybe they meant gpt3. For example, ...\n",
      "Name: Tweet, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interesting. Shows that GPT was trained on github (source for copilot). Read the thread and the comments as you cannot blindly trust AI yet. Devil is in the details, especially how you ask questions\n",
      "#ai #github #gpt3 https://t.co/rHI09ncHCB\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(df['Tweet'][0])\n",
    "print(type(df['Tweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'Series'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(analysis_sentiment(df[\u001b[39m'\u001b[39;49m\u001b[39mTweet\u001b[39;49m\u001b[39m'\u001b[39;49m]))\n",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m, in \u001b[0;36manalysis_sentiment\u001b[1;34m(tweet)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39manalysis_sentiment\u001b[39m(tweet):\n\u001b[1;32m----> 2\u001b[0m     analysis \u001b[39m=\u001b[39m TextBlob(clean_tweet(tweet))\n\u001b[0;32m      4\u001b[0m     \u001b[39mif\u001b[39;00m analysis\u001b[39m.\u001b[39msentiment\u001b[39m.\u001b[39mpolarity \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m: \u001b[39mreturn\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[0;32m      5\u001b[0m     \u001b[39melif\u001b[39;00m analysis\u001b[39m.\u001b[39msentiment\u001b[39m.\u001b[39mpolarity \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \u001b[39mreturn\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m, in \u001b[0;36mclean_tweet\u001b[1;34m(tweet)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclean_tweet\u001b[39m(tweet):\n\u001b[1;32m----> 2\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(re\u001b[39m.\u001b[39;49msub(\u001b[39m\"\u001b[39;49m\u001b[39m(@[A-Za-z0-9]+)|([^0-9A-Za-z \u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m])|(\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mw+:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mS+)\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m, tweet)\u001b[39m.\u001b[39msplit())\n",
      "File \u001b[1;32md:\\Program Fil\\Python\\Python311\\Lib\\re\\__init__.py:185\u001b[0m, in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msub\u001b[39m(pattern, repl, string, count\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, flags\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[0;32m    179\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[39m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[39m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[39m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[39m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[39m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[39mreturn\u001b[39;00m _compile(pattern, flags)\u001b[39m.\u001b[39;49msub(repl, string, count)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object, got 'Series'"
     ]
    }
   ],
   "source": [
    "print(analysis_sentiment(df['Tweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = np.array([analysis_sentiment(tweet) for tweet in df['Tweet']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                user                                              Tweet  \\\n",
      "0    HumbleTechMicro  Open $AI GPT integration by NLU company listed...   \n",
      "1    RandompileVideo  Preprint on #bioRxiv: https://t.co/BbRfEzjxt5\\...   \n",
      "2          jeffkagan  RT @JohnNosta: AI combines disparate and eclec...   \n",
      "3         heyitsnoah  Lots of folks - like @natfriedman https://t.co...   \n",
      "4        PamelaTamby  @davidlisnard Des deux... dans le sens où les ...   \n",
      "..               ...                                                ...   \n",
      "995    diogomiguel_9  🤖 ChatGPT can write ✍️ flow expressions, \\n✅ M...   \n",
      "996       Gangster5x  @MyMetaTrader #mymetatrader\\n\\n🔥 Trade smarter...   \n",
      "997        JohnNosta  GPT offers us the opportunity to customize pat...   \n",
      "998       andrew__io  #GPT3 tip of the day [bot]: Always strive to r...   \n",
      "999  RandompileVideo  Preprint on #bioRxiv: https://t.co/yiBqQnfEPz\\...   \n",
      "\n",
      "     sentiment  \n",
      "0            0  \n",
      "1            0  \n",
      "2            0  \n",
      "3            1  \n",
      "4            0  \n",
      "..         ...  \n",
      "995          1  \n",
      "996          1  \n",
      "997          1  \n",
      "998          1  \n",
      "999          0  \n",
      "\n",
      "[1000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)\n",
    "df.to_excel(r'D:\\maxim\\Documents\\Projects\\CollecteDesDonnees\\Twitter\\UserKeywordsGPT.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a805c939253dd78ab80805c33908398d8b908c1d768f626811873916a950a6f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
